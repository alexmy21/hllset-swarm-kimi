{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency Matrix (AM) as HRT (Hash Relational Tensor)\n",
    "\n",
    ">hash-addressable, dynamically-growing, PyTorch-only implementation\n",
    "\n",
    "```math\n",
    "r(hᵢ, hⱼ) ← v  (\\text{ overwrite if exists, insert if new})\n",
    "```\n",
    "\n",
    "for an arbitrary number of tokens whose only identifier is the hash value h = hash(tᵢ).\n",
    "\n",
    "\n",
    "No Python loops touch the tensor itself; all lookups and updates are O(1) and executed with one tensor op.\n",
    "\n",
    "## Data layout\n",
    "\n",
    "We keep a single 2-D tensor R of shape (capacity, capacity)\n",
    "row/col indices are internal integer ids (0,1,2,…) that we map to/from the hash values with two dictionaries:\n",
    "\n",
    "```text\n",
    "hash → idx   : self.h2i\n",
    "idx  → hash  : self.i2h\n",
    "```\n",
    "\n",
    "capacity grows automatically in powers of two (doubling when >75 % full) and the old contents are copied into the new larger tensor with a single index_copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs:\n",
      "  GPU 0: NVIDIA GeForce RTX 3060\n",
      "    Compute Capability: sm_86\n",
      "    Memory: 11.66 GB\n",
      "  GPU 1: Quadro M1200\n",
      "    Compute Capability: sm_50\n",
      "    Memory: 3.94 GB\n",
      "\n",
      "Using device: cuda:0\n",
      "Device name: NVIDIA GeForce RTX 3060\n",
      "Memory: 11.66 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexmy/SGS/SGS_lib/hllset-swarm-kimi/.venv/lib64/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU1 Quadro M1200 which is of cuda capability 5.0.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/home/alexmy/SGS/SGS_lib/hllset-swarm-kimi/.venv/lib64/python3.12/site-packages/torch/cuda/__init__.py:304: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  warnings.warn(matched_cuda_warn.format(matched_arches))\n",
      "/home/alexmy/SGS/SGS_lib/hllset-swarm-kimi/.venv/lib64/python3.12/site-packages/torch/cuda/__init__.py:326: UserWarning: \n",
      "Quadro M1200 with CUDA capability sm_50 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the Quadro M1200 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Check available GPUs\n",
    "print(\"Available GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"  GPU {i}: {props.name}\")\n",
    "    print(f\"    Compute Capability: sm_{props.major}{props.minor}\")\n",
    "    print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Select RTX 3060 (adjust index based on output above)\n",
    "# Option A: Hide Quadro, only show RTX\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Assuming RTX is at index 1\n",
    "DEVICE = torch.device(\"cuda:0\")  # Now index 0 refers to the RTX\n",
    "\n",
    "# Option B: Explicitly select by index (if you don't set CUDA_VISIBLE_DEVICES)\n",
    "# DEVICE = torch.device(\"cuda:1\")  # Direct access to RTX at original index 1\n",
    "\n",
    "print(f\"\\nUsing device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(DEVICE)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(DEVICE).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hllset_swarm.hrt import HRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9000, device='cuda:0')\n",
      "tensor([[0.0000, 0.9000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "rel = HRT(device=dev)\n",
    "\n",
    "tok_hashes = [hash(t) for t in [\"cat\", \"dog\", \"bird\"]]\n",
    "\n",
    "rel.update(tok_hashes[0], tok_hashes[1], 0.8)   # r(cat,dog)=0.8\n",
    "rel.update(tok_hashes[1], tok_hashes[2], 0.5)   # r(dog,bird)=0.5\n",
    "rel.update(tok_hashes[0], tok_hashes[1], 0.9)   # overwrite with 0.9\n",
    "\n",
    "print(rel.get(tok_hashes[0], tok_hashes[1]))    # tensor(0.9, device='cuda:0')\n",
    "print(rel.get_dense())                          # 3×3 dense sub-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{7604630207966975244: 0.699999988079071, 6685043807368327666: 0.5}\n",
      "{-4822480586015876194: 0.8999999761581421, -6830413446406971881: 0.5}\n",
      "tensor([[0.0000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "rel = HRT()\n",
    "\n",
    "# build a tiny time graph\n",
    "t = [hash(w) for w in [\"t0\", \"t1\", \"t2\", \"t3\"]]\n",
    "rel.update(t[0], t[1], 0.9)   # t0 → t1\n",
    "rel.update(t[1], t[2], 0.8)   # t1 → t2\n",
    "rel.update(t[2], t[3], 0.7)   # t2 → t3\n",
    "rel.update(t[0], t[3], 0.5)   # t0 → t3  (skip connection)\n",
    "\n",
    "print(rel.get_history(t[3], horizon=2))\n",
    "# {hash(t2): 0.7, hash(t0): 0.5}   two strongest predecessors\n",
    "\n",
    "print(rel.get_future(t[0], horizon=2))\n",
    "# {hash(t1): 0.9, hash(t3): 0.5}   two strongest successors\n",
    "\n",
    "print(rel.project([t[0], t[2], t[3]]))\n",
    "# 3×3 dense tensor of relations among exactly these three tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Demo\n",
    "\n",
    "### Complexity\n",
    "\n",
    "Time: O(|keep|²) – once.\n",
    "Memory: the old matrix is discarded immediately; only the pruned one remains.\n",
    "All subsequent operations (update, get_history, project, …) run on the smaller matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before prune torch.Size([16, 16])\n",
      "after prune torch.Size([3, 3])\n",
      "{-9176445229405775260: 0, 7944667499784224323: 1, -9135990848321305430: 2}\n",
      "{-9176445229405775260: 4.0}\n"
     ]
    }
   ],
   "source": [
    "rel = HRT()\n",
    "\n",
    "# build a graph with 4 tokens\n",
    "t = [hash(w) for w in [\"A\", \"B\", \"C\", \"D\"]]\n",
    "rel.update(t[0], t[1], 1.0)\n",
    "rel.update(t[1], t[2], 2.0)\n",
    "rel.update(t[2], t[3], 3.0)\n",
    "rel.update(t[0], t[3], 4.0)\n",
    "\n",
    "print(\"before prune\", rel.R.shape)      # 4×4\n",
    "\n",
    "pruned = rel.prune([t[0], t[2], t[3]])  # drop B\n",
    "print(\"after prune\", pruned.shape)      # 3×3\n",
    "print(rel.h2i)                          # {hash_A:0, hash_C:1, hash_D:2}\n",
    "\n",
    "# history/future now work only on the pruned set\n",
    "print(rel.get_history(t[3]))            # {hash_A:4.0, hash_C:3.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity\n",
    "\n",
    "- update/get: O(1) average time, one tensor write.\n",
    "- memory: O(N²) where N = number of distinct hashes (only the actually used N×N block is returned by get_dense()).\n",
    "- growth: amortised O(1); when capacity doubles we pay one index_copy of the existing N×N block.\n",
    "\n",
    "## Extensions\n",
    "\n",
    "1. Symmetric relation: mirror each write R[j,i] = R[i,j].\n",
    "2. Directed vs undirected: store only upper triangle and use a triu_indices view.\n",
    "3. Mini-batch updates: vectorise with index_put_ and two integer tensors rows, cols."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
