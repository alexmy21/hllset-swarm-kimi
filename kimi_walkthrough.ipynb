{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency Matrix (AM) as HRT (Hash Relational Tensor)\n",
    "\n",
    ">hash-addressable, dynamically-growing, PyTorch-only implementation\n",
    "\n",
    "```math\n",
    "r(hᵢ, hⱼ) ← v  (\\text{ overwrite if exists, insert if new})\n",
    "```\n",
    "\n",
    "for an arbitrary number of tokens whose only identifier is the hash value h = hash(tᵢ).\n",
    "\n",
    "\n",
    "No Python loops touch the tensor itself; all lookups and updates are O(1) and executed with one tensor op.\n",
    "\n",
    "## Data layout\n",
    "\n",
    "We keep a single 2-D tensor R of shape (capacity, capacity)\n",
    "row/col indices are internal integer ids (0,1,2,…) that we map to/from the hash values with two dictionaries:\n",
    "\n",
    "```text\n",
    "hash → idx   : self.h2i\n",
    "idx  → hash  : self.i2h\n",
    "```\n",
    "\n",
    "capacity grows automatically in powers of two (doubling when >75 % full) and the old contents are copied into the new larger tensor with a single index_copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexmy/SGS/SGS_lib/hllset-swarm-kimi/.venv/lib64/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU1 Quadro M1200 which is of cuda capability 5.0.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/home/alexmy/SGS/SGS_lib/hllset-swarm-kimi/.venv/lib64/python3.12/site-packages/torch/cuda/__init__.py:304: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  warnings.warn(matched_cuda_warn.format(matched_arches))\n",
      "/home/alexmy/SGS/SGS_lib/hllset-swarm-kimi/.venv/lib64/python3.12/site-packages/torch/cuda/__init__.py:326: UserWarning: \n",
      "Quadro M1200 with CUDA capability sm_50 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the Quadro M1200 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Check available GPUs\n",
    "# print(\"Available GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "#     print(f\"  GPU {i}: {props.name}\")\n",
    "#     print(f\"    Compute Capability: sm_{props.major}{props.minor}\")\n",
    "#     print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Select RTX 3060 (adjust index based on output above)\n",
    "# Option A: Hide Quadro, only show RTX\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Assuming RTX is at index 1\n",
    "DEVICE = torch.device(\"cuda:0\")  # Now index 0 refers to the RTX\n",
    "\n",
    "# Option B: Explicitly select by index (if you don't set CUDA_VISIBLE_DEVICES)\n",
    "DEVICE = torch.device(\"cuda:1\")  # Direct access to RTX at original index 1\n",
    "\n",
    "# print(f\"\\nUsing device: {DEVICE}\")\n",
    "# if DEVICE.type == \"cuda\":\n",
    "#     print(f\"Device name: {torch.cuda.get_device_name(DEVICE)}\")\n",
    "#     print(f\"Memory: {torch.cuda.get_device_properties(DEVICE).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "### Complexity\n",
    "\n",
    "Time: O(|keep|²) – once.\n",
    "Memory: the old matrix is discarded immediately; only the pruned one remains.\n",
    "All subsequent operations (update, get_history, project, …) run on the smaller matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from src.hllset_swarm.swarm_hrt import SwarmHRT\n",
    "from src.hllset_swarm.constants import HASH_FUNC\n",
    "from src.hllset_swarm.ingest import ingest_stream\n",
    "# from src.hllset_swarm import SwarmHRT, ingest_stream, HASH_FUNC, P_BITS, SHARED_SEED\n",
    "\n",
    "def demo():\n",
    "    hrt = SwarmHRT(max_edges=20_000)\n",
    "    corpus = [\"人工智能引领未来\", \"机器学习改变世界\", \"深度学习驱动创新\"]\n",
    "    commits = []\n",
    "    # belief = torch.zeros(len(hrt.h2i), device=\"cuda\", dtype=torch.float32)\n",
    "    for belief_vec, commit in ingest_stream(corpus, hrt, swarm_iters_per_chunk=3):\n",
    "        commits.append(commit)\n",
    "\n",
    "    belief = torch.zeros(len(hrt.h2i), device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "    # ---- guided finish ----\n",
    "    dest_hashes = [HASH_FUNC(c) for c in [\"未\", \"来\", \"世\", \"界\"]]\n",
    "    p_star = torch.zeros(len(hrt.h2i), device=\"cuda\", dtype=torch.float32)\n",
    "    for h in dest_hashes:\n",
    "        if h in hrt.h2i:\n",
    "            p_star[hrt.h2i[h]] = 1.0\n",
    "    p_star = p_star / p_star.sum()\n",
    "    for step in range(5):\n",
    "        belief = hrt.guided_step(belief, p_star)\n",
    "        if torch.norm(belief - p_star, 1) < 1e-3:\n",
    "            break\n",
    "    top = torch.topk(belief, 18).indices\n",
    "    # tokens = [hrt.i2h[i.item()] for i in top]\n",
    "    tokens = [hrt.h2token.get(hrt.i2h[i.item()], f\"<unk_{i.item()}>\") for i in top]\n",
    "\n",
    "    print(\"Destination tokens:\", \"\".join(tokens))\n",
    "    Path(\"kimi_git_log.json\").write_text(json.dumps(commits, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Destination tokens: 领未领未来工智工智能人工人人工智工能引领能引智能引能引领引引领未领智能智\n"
     ]
    }
   ],
   "source": [
    "demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity\n",
    "\n",
    "- update/get: O(1) average time, one tensor write.\n",
    "- memory: O(N²) where N = number of distinct hashes (only the actually used N×N block is returned by get_dense()).\n",
    "- growth: amortised O(1); when capacity doubles we pay one index_copy of the existing N×N block.\n",
    "\n",
    "## Extensions\n",
    "\n",
    "1. Symmetric relation: mirror each write R[j,i] = R[i,j].\n",
    "2. Directed vs undirected: store only upper triangle and use a triu_indices view.\n",
    "3. Mini-batch updates: vectorise with index_put_ and two integer tensors rows, cols."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
